{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14db6f54",
   "metadata": {},
   "source": [
    "![AUEB.png](AUEB.png) ![MSc_BA.png](MSc_BA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda0a08",
   "metadata": {},
   "source": [
    "# Athens University of Economics and Business\n",
    "# School of Business\n",
    "# Department of Management Science & Technology\n",
    "# Master of Science in Business Analytics\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c74b6",
   "metadata": {},
   "source": [
    "<table style='float:left;font-size: 20px;'>\n",
    "    <tr>\n",
    "        <th style='text-align: left;'>Program:</th>\n",
    "        <td style='text-align: left;'>Full-Time</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th style='text-align: left;'>Quarter:</th>\n",
    "        <td style='text-align: left;'>3rd (Spring Quarter)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th style='text-align: left;'>Course:</th>\n",
    "        <td style='text-align: left;'>Advanced Topics in Data Engineering</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th style='text-align: left;'>Instructor:</th>\n",
    "        <td style='text-align: left;'>Dr. Giorgos Alexiou</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th style='text-align: left;'>Assignment:</th>\n",
    "        <td style='text-align: left;'>Entity Resolution Assignment</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <th style='text-align: left;'>Student (Registration No):</th>\n",
    "        <td style='text-align: left;'>Souflas Eleftherios-Efthymios (f2822217)</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4376e41",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3360c",
   "metadata": {},
   "source": [
    "## Task A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd5ca2",
   "metadata": {},
   "source": [
    "A **Token Blocking** methodology was implemented in a schema-agnostic manner to create blocks represented as Key-Value (K-V) pairs from the provided `ER-Data.csv` file. The primary goal of this approach was to create distinct Blocking Keys (BK) derived from entities' attribute values, excluding the id column, to facilitate accurate matching and comprehensive analysis.\n",
    "\n",
    "The required libraries (`pandas` and `html`) were imported and from some other libraries (`nltk`, `pprint`, and `contextlib`) some of their functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c10af3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "import html\n",
    "from contextlib import redirect_stdout\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66689b0",
   "metadata": {},
   "source": [
    "Two functions are defined: `token_blocking` and `decode_html_special_chars`.\n",
    "\n",
    "1. `token_blocking(dataframe)`:\n",
    "   This function takes a DataFrame as input and returns an inverted index for the data. The inverted index is implemented as a Python dictionary, where the keys are tokens (words) extracted from the DataFrame attributes, and the values are sets of corresponding IDs (unique identifiers) of the DataFrame's entities associated with each token. The function processes the DataFrame row by row, extracting the ID and all other attribute values from each row. For each attribute value, the function converts the text to lowercase, tokenizes it into individual words, and filters out English stopwords using the NLTK library. It then populates the inverted index by associating each token with the set of IDs of entities containing that token. Finally, the function removes tokens from the index that do not meet the minimum threshold of having at least two entities associated with them. The resulting inverted index, excluding the tokens with insufficient entities, is returned as a dictionary.\n",
    "\n",
    "2. `decode_html_special_chars(text)`:\n",
    "   This function takes a text input and decodes any HTML special characters present in the text using the `html.unescape` function from the `html` module. It specifically targets instances of encoded characters like '&amp;', '&lt;', '&gt;', etc., converting them back to their original form. The function is designed to handle both strings and other data types. If the input is a string, it decodes the HTML special characters and returns the updated string. For other data types, it simply returns the original input.\n",
    "\n",
    "Tokens that belong to the english list of stopwords, like I, you, of, and, etc., are filtered out. If this step was omitted, the token with the most entities as its set of values would be the 'of' token (word) having 30,326 entities associated to it. After the stopwords filtering step, 'j' is the token with the most values in its block, having 7,599 entities, reducing this way the computation of unnecessary comparisons.\n",
    "\n",
    "Overall, these functions work together to create an inverted index for a given DataFrame while also providing a utility function to decode any HTML special characters present in the data. The inverted index is a useful data structure for efficient record linkage and entity resolution tasks, allowing for fast lookups of entities associated with specific attributes or tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a030f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function token_blocking that takes a DataFrame as input and returns an inverted index\n",
    "def token_blocking(dataframe):\n",
    "    # Create an empty dictionary to store the blocking index\n",
    "    index = {}\n",
    "    # A list of English stopwords from nltk.corpus\n",
    "    en_stop = stopwords.words('english')\n",
    "    # Iterate over DataFrame's rows as Pandas named tuples without returning the index as the first element of the tuple\n",
    "    for row in dataframe.itertuples(index=False):\n",
    "        # Extract the ID and all other attribute values from the row tuple\n",
    "        id_, *attributes = row\n",
    "        # Iterate over each attribute value\n",
    "        for attr in attributes:\n",
    "            # Check if the attribute value is indeed available (not null)\n",
    "            if pd.notna(attr):\n",
    "                # Transform the attribute value to lowercase and split into individual words\n",
    "                tokens = str(attr).lower().split()\n",
    "                # For each token in the attribute's value add the ID to the corresponding blocking key's value set\n",
    "                for token in tokens:\n",
    "                    # Filter out stopwords\n",
    "                    if token not in en_stop:\n",
    "                        # If the token is not in the index, create a new entry with the ID as the value set\n",
    "                        if token not in index:\n",
    "                            index[token] = {id_}\n",
    "                        # If the token already exists in the index, add the ID to its value set\n",
    "                        else:\n",
    "                            index[token].add(id_)\n",
    "    # Create a list to store keys to delete\n",
    "    keys_to_delete = []\n",
    "    # Iterate over each value (set of IDs) in the blocking index\n",
    "    for key, value in index.items():\n",
    "        # Each block should contain at least two entities, otherwise add the key to the keys_to_delete list\n",
    "        if len(value) < 2:\n",
    "            keys_to_delete.append(key)\n",
    "    # Delete the keys along with their values from the dictionary\n",
    "    for key in keys_to_delete:\n",
    "        del index[key]\n",
    "    # Return the blocking index as a dictionary\n",
    "    return index\n",
    "\n",
    "\n",
    "# Function to decode HTML special characters in a cell\n",
    "def decode_html_special_chars(text):\n",
    "    return html.unescape(text) if isinstance(text, str) else text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0808a3",
   "metadata": {},
   "source": [
    "The `ER-Data.csv` file was loaded into a pandas DataFrame. The file must be placed in the 'Data' folder. The `pandas` library is utilized to read the CSV file into a DataFrame named 'df', using the semicolon (;) separator to parse the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c870e518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "data_path = \"../Data/ER-Data.csv\"\n",
    "df = pd.read_csv(data_path, sep=\";\")\n",
    "print(\"Data loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0542bba1",
   "metadata": {},
   "source": [
    "Then, the task of removing HTML special characters from the DataFrame 'df' is being executed. This includes characters such as `&amp;`, `&hellip;`, `&lt;`, and `&quot;`. The aim is to decode the HTML entities and replace them with their corresponding characters to ensure proper representation of the textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a58af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing HTML special characters...\n",
      "Completed successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove HTML special characters, like `&amp;`, `&hellip;`, `&lt;`, `&quot;`\n",
    "print(\"Removing HTML special characters...\")\n",
    "# Apply the function to the entire DataFrame\n",
    "df = df.applymap(decode_html_special_chars)\n",
    "print(\"Completed successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49170383",
   "metadata": {},
   "source": [
    "The task of token blocking on the DataFrame `df` is being executed. Token blocking is a schema-agnostic approach used to create blocks in the form of Key-Value (K-V) pairs. Each distinct Blocking Key (BK) is derived from the entities' attribute values, excluding the 'ID' column, which is used only as a reference for the blocking index creation and not included in the blocking process. The `token_blocking(df)` function takes the DataFrame `df` as input and returns an inverted index (blocking index) containing Blocking Keys as keys and sets of corresponding entity 'id's as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bec90e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Token Blocking...\n",
      "Completed successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform Token Blocking\n",
    "print(\"Performing Token Blocking...\")\n",
    "block_index = token_blocking(df)\n",
    "print(\"Completed successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6ad3a9",
   "metadata": {},
   "source": [
    "Finally, the resulting Blocking Index is presented in a neatly formatted manner to the output. The purpose of displaying the index is to provide a clear and organized view of the relationships formed through token blocking. The format of the displayed output is as follows:\n",
    "\n",
    "```\n",
    "{'Blocking Key': {Entities}}\n",
    "--------------------------\n",
    "{Block_1: {Entity_1, Entity_2, ...},\n",
    " Block_2: {Entity_3, Entity_4, ...},\n",
    " ...\n",
    " Block_N: {Entity_M, Entity_N, ...}}\n",
    "```\n",
    "\n",
    "Each 'Blocking Key' is shown as a dictionary key, and the corresponding 'Entities' associated with that key are shown as a set of IDs enclosed in curly braces. The displayed output uses proper indentation and alignment for improved readability. The pprint function is used to format the output, ensuring that the representation remains concise and well-structured even for large blocking indices. Below a sample of 10 mid-sized blocks of the blocking index is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa893f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample print of 10 mid-size blocks\n",
      "{'Blocking Key': {Entities}}\n",
      "----------------------------\n",
      "{'advising': {5, 7215, 17067, 17455, 22627, 42038, 46748, 52148,\n",
      "              52401, 54359, 63637},\n",
      " 'facilities': {4, 769, 1651, 2425, 4509, 5481, 8833, 9626, 11207,\n",
      "                14222, 15146, 18392, 29246, 29260, 33357, 34649,\n",
      "                34828, 35711, 35972, 37287, 38109, 40835, 41463,\n",
      "                43646, 46920, 50229, 57607, 59493, 61007, 62757,\n",
      "                62788, 65983},\n",
      " 'hansen,': {3, 3970, 10387, 13434, 15901, 19173, 20234, 23716, 24927,\n",
      "             38317, 39985, 41534, 42422, 43897, 49003, 53711, 56426,\n",
      "             57778, 59313, 62493},\n",
      " 'inc': {1, 923, 2857, 3057, 3486, 4378, 5589, 6339, 7038, 8574, 9368,\n",
      "         10500, 11596, 15004, 17005, 18337, 21912, 22216, 22275,\n",
      "         23987, 24308, 26475, 27244, 29327, 30987, 32596, 36256,\n",
      "         36411, 38590, 39000, 40028, 41393, 42111, 42685, 43073,\n",
      "         43918, 44647, 44908, 45497, 46763, 47124, 49406, 50987,\n",
      "         51988, 52505, 53149, 56545, 56805, 57323, 58857, 59169,\n",
      "         60213, 61288, 61397, 62978, 65238},\n",
      " 'infectious': {4, 1127, 22840, 26202, 38006, 40890, 45665, 48973,\n",
      "                54199, 64194},\n",
      " 'initiation': {2, 2653, 8485, 15660, 15801, 17496, 21279, 22138,\n",
      "                25349, 31892, 34902, 37086, 37763, 42392, 42454,\n",
      "                49508},\n",
      " 'labelling': {3, 12452, 18804, 18928, 20926, 36095, 36814, 42629,\n",
      "               43414, 51275, 57566},\n",
      " 'road': {1, 588, 835, 1865, 2075, 2414, 7096, 9041, 9470, 10515,\n",
      "          14530, 14759, 16568, 17834, 18431, 19955, 19961, 25250,\n",
      "          27089, 28273, 28548, 29054, 29806, 31596, 31828, 32742,\n",
      "          32876, 36420, 36911, 38165, 38707, 41916, 42167, 42329,\n",
      "          42867, 43779, 44030, 44123, 45497, 45715, 45920, 48309,\n",
      "          50056, 50419, 52812, 53149, 54551, 55470, 55873, 58270,\n",
      "          58680, 62054},\n",
      " 'valley': {1, 7798, 8932, 16285, 21712, 27852, 28464, 28783, 30722,\n",
      "            32175, 33057, 33085, 33155, 42019, 45850, 46269, 47380,\n",
      "            57933, 59594},\n",
      " 'wm': {6, 1068, 1519, 1648, 2663, 3089, 3380, 3767, 4507, 5293, 5733,\n",
      "        6981, 7257, 7420, 8447, 9024, 9893, 10497, 11678, 12668,\n",
      "        12717, 13252, 13368, 14718, 15951, 16891, 16975, 18268, 19871,\n",
      "        20224, 20265, 21626, 22312, 22777, 24366, 25126, 25452, 25705,\n",
      "        25994, 28292, 30260, 30477, 31417, 31632, 33218, 33856, 33990,\n",
      "        36260, 36275, 36871, 36995, 39378, 40622, 40908, 41309, 41789,\n",
      "        41951, 42049, 42096, 42741, 44810, 45082, 46466, 47826, 49342,\n",
      "        49556, 49629, 50084, 50544, 51046, 52188, 52204, 53325, 53581,\n",
      "        53800, 54774, 57315, 57981, 58080, 58915, 59847, 59873, 60511,\n",
      "        62104, 62290, 62402, 62842}}\n"
     ]
    }
   ],
   "source": [
    "# Create a list to store keys to print as sample\n",
    "keys_to_print = []\n",
    "# Iterate over each value (set of IDs) in the blocking index\n",
    "for token_key, ID_value in block_index.items():\n",
    "    # Each block should contain at least two entities, otherwise add the key to the keys_to_delete list\n",
    "    if 10 <= len(ID_value) <= 100:\n",
    "        keys_to_print.append(token_key)\n",
    "    if len(keys_to_print) == 10:\n",
    "        break\n",
    "# Create a new dictionary containing only the desired keys and values\n",
    "filtered_dict = {key: value for key, value in block_index.items() if key in keys_to_print}\n",
    "# Display the sample index\n",
    "print(\"Sample print of 10 mid-size blocks\")\n",
    "print(\"{'Blocking Key': {Entities}}\")\n",
    "print(\"----------------------------\")\n",
    "pprint(filtered_dict, width=70, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068c367a",
   "metadata": {},
   "source": [
    "The resulting blocking index, obtained after performing Token Blocking on a DataFrame, is also written to a file named `blocking_index_print.txt`. The purpose of writing to this file is to store the entire output of the blocking index, which may be quite extensive, and allow users to view it conveniently at a later time, if needed.\n",
    "The resulting output is directed to the file `blocking_index_print.txt` using the `redirect_stdout` context manager to capture the standard output and store it in the file. Finally, the file is closed, and the process is completed, confirming that the data has been successfully written to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d6937f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing to a file, named `blocking_index_print.txt`, to be able to see the entire output if needed...\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "# Write the resulting index to a file\n",
    "print(\"\\nWriting to a file, named `blocking_index_print.txt`, to be able to see the entire output if needed...\")\n",
    "f = open('blocking_index_print.txt', 'w', encoding=\"utf-8\")\n",
    "with redirect_stdout(f):\n",
    "    pprint(block_index, width=120, compact=True)\n",
    "f.close()\n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a841fc4",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8171a874",
   "metadata": {},
   "source": [
    "## Task B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de99ef7d",
   "metadata": {},
   "source": [
    "Task B involves the **computation of all possible comparisons** required to resolve duplicates within the blocks created in Task A. To achieve this, the number of comparisons for each block in the `block_index` is calculated. A function named `calculate_comparisons` is defined, which takes a blocking index as input. The purpose of this function is to compute the total number of pairwise comparisons required for the entities within the given blocking data structure.\n",
    "\n",
    "The function employs the combination formula (`comb` function) from the `math` module to calculate the number of unique pairs that can be formed within each block. It iterates through the values (i.e., sets of IDs) within the blocking index to determine the number of distinct entities present in each block and by computing their lengths, it effectively counts the unique entities. For each block, the number of pairwise comparisons possible among its entities is determined using the combination formula. The formula calculates the number of ways to choose two entities from the block without repetition. These pairwise combinations represent the possible comparisons that can be made within the block.\n",
    "\n",
    "The function accumulates the number of pairwise comparisons within each block and aggregates them to obtain the total number of comparisons required for the entire blocking data structure. This total count represents the final result of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2300c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the combination formula from the math module\n",
    "from math import comb\n",
    "\n",
    "\n",
    "# Define the function calculate_comparisons that takes a blocking index as input\n",
    "def calculate_comparisons(blocking_index):\n",
    "    # Initialize a variable to keep track of the total number of comparisons\n",
    "    total_comparisons = 0\n",
    "    # Iterate over each value (set of IDs) in the blocking index\n",
    "    for value in blocking_index.values():\n",
    "        # Calculate the number of unique entities in the block by getting the set's length\n",
    "        num_entities = len(value)\n",
    "        # Calculate the number of pairwise comparisons that can be made within the block\n",
    "        # using the combination formula from the math module (math.comb)\n",
    "        comparisons_in_block = comb(num_entities, 2)\n",
    "        # Add the number of comparisons within this block to the total number of comparisons\n",
    "        total_comparisons += comparisons_in_block\n",
    "    # Return the total number of comparisons across all blocks in the blocking index\n",
    "    return total_comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35e1132",
   "metadata": {},
   "source": [
    "After the `calculate_comparisons` function is executed with the given blocking index `block_index`, the total number of pairwise comparisons is obtained and printed (392,309,099 comparisons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47e65ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating all pair-wise comparisons...\n",
      "Number of comparisons: 392309099\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of comparisons for the given blocking index (block_index)\n",
    "print(\"Calculating all pair-wise comparisons...\")\n",
    "num_comparisons = calculate_comparisons(block_index)\n",
    "# Print the result, which represents the total number of pair-wise comparisons\n",
    "print(\"Number of comparisons:\", num_comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98183da",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7972a1fd",
   "metadata": {},
   "source": [
    "## Task C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4870e6ed",
   "metadata": {},
   "source": [
    "In this Task, a custom (self-created) **Meta-Blocking Graph** was created from the Block Collection created in the first Task.\n",
    "The Graph was stored in a `SQLite` database (disk) instead of creating it in memory using libraries, like `NetworkX` or `iGraph`, because when executing the latter (with `NetworkX`), after 30 minutes of run-time, a memory error was being produced as the entire graph could not fit in memory. \n",
    "\n",
    "The Meta-Blocking procedure contained the following phases:\n",
    "\n",
    "1. <u>Graph Building</u>: It took 10 minutes to complete the insertion of all nodes (entities) and undirected edges (pairs of co-occurring entities) to the SQLite (\"graph\") database for every block of the Block Colection created in Task A.\n",
    "2. <u>Edge Weighting</u>: It took approximately 14 minutes to apply the attribute-agnostic CBS (Common Blocks Scheme) Weighting Scheme in order to put weights on the previously created edges, based on the number of common blocks (edges) that entities per comparison have in common.\n",
    "3. <u>Graph Pruning</u>: An Edge-centric pruning algorithm was implemented by deleting all edges having weight < 2.\n",
    "4. <u>Block Collecting</u>: The pruned blocking graph was transformed into a new block collection. Every retained edge created a block of minimum size. The final number of comparisons of the new block collection (after edge pruning) was calculated and printed (53,626,150 comparisons).\n",
    "\n",
    "Firstly, a class, named `SQLiteGraph`, is defined, that facilitates interactions with an SQLite database to represent a graph. The class contains different methods for facilitating all phases of the Meta-Blocking procedure:\n",
    "\n",
    "1. The class includes a constructor (`__init__`) that takes a parameter representing the path to an SQLite database file. This establishes a connection to the specified database.\n",
    "\n",
    "2. The method `add_nodes_and_edges_from_token_blocking_dict` is responsible for populating the database with nodes and edges. It receives a blocking index as input and iterates over the index's values (which represent sets of entity IDs). For each set of entity IDs, the method calculates pairs of unique combinations within the set using `itertools.combinations` and inserts these pairs as edges into the 'edgelist' table of the database. Additionally, the unique entity IDs are inserted into the 'nodelist' table. Throughout this process, a progress bar is used to track the insertion progress. The method updates the progress bar as pairs of edges are inserted and nodes are added to the database. Once all data is inserted, the method commits the changes to the database and closes the progress bar.\n",
    "\n",
    "3. The method `query_graph(self, query)` facilitates running SELECT queries on the SQLite database. It accepts a SQL query as input, establishes a connection to the database, executes the query, fetches all the rows from the query result, and returns them as a list.\n",
    "\n",
    "4. The method `is_num_edges_equal_to(self, number_comparisons)` checks whether the number of edges (comparisons) in the graph (retrieved from the 'edgelist' table) is equal to a specified number (`number_comparisons`). It does so, by executing a SELECT query to count the number of rows in the 'edgelist' table and comparing it with the provided number. The method returns a boolean indicating the result of the comparison.\n",
    "\n",
    "5. The method `apply_cbs_weighting_scheme(self)` applies the Common Blocks Scheme (CBS) weighting to the edges in the graph. It establishes a connection to the database, creates a new table named 'edgelist_weighted' by aggregating and calculating the weights of the edges from the original 'edgelist' table. The weights are computed based on the count of similar edges (common blocks). The original 'edgelist' table is then dropped, and the new 'edgelist_weighted' table is renamed to 'edgelist' to replace it. The method provides progress tracking using a progress bar.\n",
    "\n",
    "6. The method `prune_edges_with_weight_lower_than(self, limit)` prunes edges in the graph whose weight is lower than a specified limit. It establishes a connection to the database, defines a query to delete edges with weight below the given limit, and executes the query. The changes are then committed to the database.\n",
    "\n",
    "7. The method `number_of_edges(self)` retrieves the total number of edges in the graph. It defines a SELECT query to count the number of rows (edges) in the 'edgelist' table, executes the query using the `query_graph` method, and returns the count of edges extracted from the query result.\n",
    "\n",
    "8. The method `number_of_nodes(self)` retrieves the total number of nodes (entities) in the graph. It defines a SELECT query to count the number of rows (nodes) in the 'nodelist' table, executes the query using the `query_graph` method, and returns the count of nodes extracted from the query result.\n",
    "\n",
    "9. The method `close_connection(self)` closes the connection established with the SQLite database and associated with the instance of the `SQLiteGraph` class.\n",
    "\n",
    "This class serves as a tool to efficiently manage the process of constructing a graph in an SQLite database by adding nodes and edges based on a provided blocking index. It encapsulates the necessary database interactions and progress tracking, enabling querying the graph, comparing the number of edges, applying a CBS weighting scheme to the edges within the SQLite database, allowing the retrieval of edge and node information from the SQLite database and providing a mechanism to close the database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5df73c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from itertools import combinations\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class SQLiteGraph:\n",
    "\n",
    "    # Constructor for the SQLiteGraph class\n",
    "    def __init__(self, database_file):\n",
    "        # Initialize a connection to an SQLite database using the provided file path\n",
    "        # The database_file parameter represents the path to the SQLite database file\n",
    "        # This connection will be used to interact with the SQLite database\n",
    "        self.conn = sqlite3.connect(database_file)\n",
    "\n",
    "    # Function to add nodes and edges to the SQLite database from a blocking index\n",
    "    def add_nodes_and_edges_from_token_blocking_dict(self, blocking_index):\n",
    "        # Establish a connection to the SQLite database\n",
    "        conn = self.conn\n",
    "        c = conn.cursor()\n",
    "        # Drop the 'nodelist' and 'edgelist' tables if they exist\n",
    "        c.execute('''DROP TABLE IF EXISTS nodelist''')\n",
    "        c.execute('''DROP TABLE IF EXISTS edgelist''')\n",
    "        # Create the 'nodelist' table if it doesn't exist\n",
    "        c.execute('''CREATE TABLE IF NOT EXISTS nodelist (id INTEGER)''')\n",
    "        # Create the 'edgelist' table if it doesn't exist\n",
    "        c.execute('''CREATE TABLE IF NOT EXISTS edgelist\n",
    "                     (source INTEGER, target INTEGER)''')\n",
    "        # Begin a transaction\n",
    "        c.execute('BEGIN TRANSACTION')\n",
    "        # Progress bar to track the insertion progress\n",
    "        pbar = tqdm(total=num_comparisons, desc='Progress')\n",
    "        # Initialize an empty set to store unique nodes (entities)\n",
    "        node_set = set()\n",
    "        # Iterate over each value (list of IDs) in the blocking index\n",
    "        for value in blocking_index.values():\n",
    "            # Get the union (distinct IDs) of the sets node_set and the IDs contained in each block\n",
    "            # and store it in the node_set\n",
    "            node_set.union(value)\n",
    "            # Use itertools.combinations to generate unique pairs within the current block\n",
    "            block_pairs = set(combinations(value, 2))\n",
    "            # Insert the pairs into the 'edgelist' table in the database\n",
    "            c.executemany('INSERT INTO edgelist (source, target) VALUES (?, ?)', block_pairs)\n",
    "            # Update the progress bar to reflect the number of inserted pairs\n",
    "            pbar.update(len(block_pairs))\n",
    "            # Clear the block_pairs set for memory efficiency\n",
    "            block_pairs.clear()\n",
    "        # Insert the unique nodes into the 'nodelist' table in the database\n",
    "        c.executemany('INSERT INTO nodelist (id) VALUES (?)', node_set)\n",
    "        # Commit the changes to the database\n",
    "        conn.commit()\n",
    "        # Set the progress bar description to 'Committing...' before closing it\n",
    "        pbar.set_description('Committing...')\n",
    "        # Close the progress bar\n",
    "        pbar.set_description('Completed')\n",
    "        pbar.close()\n",
    "\n",
    "    # Function to run SELECT query on the SQLite database\n",
    "    def query_graph(self, query):\n",
    "        # Establish a connection to the SQLite database\n",
    "        conn = self.conn\n",
    "        c = conn.cursor()\n",
    "        # Execute the SELECT query\n",
    "        c.execute(query)\n",
    "        # Fetch all rows from the query result\n",
    "        rows = c.fetchall()\n",
    "        # Return the query result\n",
    "        return rows\n",
    "\n",
    "    # Function to check if the number of edges in the graph (comparisons) is the same as a specified number\n",
    "    def is_num_edges_equal_to(self, number_comparisons):\n",
    "        # Define the SELECT query to count the number of rows (edges) in the 'edgelist' table\n",
    "        query = 'SELECT COUNT(*) FROM edgelist'\n",
    "        # Call the function to run the SELECT query on the graph database and retrieve the result\n",
    "        result = self.query_graph(query)\n",
    "        # Return a boolean indicating whether the number of edges in the database is equal to 'number_comparisons'\n",
    "        return result[0][0] == number_comparisons\n",
    "\n",
    "    # Function to apply Common Blocks Scheme (CBS) weights to the edges\n",
    "    def apply_cbs_weighting_scheme(self):\n",
    "        # Establish a connection to the SQLite database\n",
    "        conn = self.conn\n",
    "        c = conn.cursor()\n",
    "        # Progress bar initialization and update\n",
    "        pbar = tqdm(total=4, desc='Progress', mininterval=0.01)\n",
    "        pbar.update(1)\n",
    "        # Define the SELECT query to create a new table 'edgelist_weighted' with weighted edges\n",
    "        c.execute('CREATE TABLE edgelist_weighted AS \\\n",
    "        SELECT source, target, COUNT(*) as weight FROM edgelist GROUP BY source, target')\n",
    "        pbar.update(1)\n",
    "        # Drop the existing 'edgelist' table to replace it with the weighted version\n",
    "        c.execute('DROP TABLE edgelist')\n",
    "        pbar.update(1)\n",
    "        # Rename the 'edgelist_weighted' table to 'edgelist'\n",
    "        c.execute('ALTER TABLE edgelist_weighted RENAME TO edgelist')\n",
    "        pbar.update(1)\n",
    "        # Commit the changes to the database\n",
    "        pbar.set_description('Committing...')\n",
    "        conn.commit()\n",
    "        # Close the progress bar\n",
    "        pbar.set_description('Completed')\n",
    "        pbar.close()\n",
    "\n",
    "    # Function to prune edges if their weight is lower than a specified limit\n",
    "    def prune_edges_with_weight_lower_than(self, limit):\n",
    "        # Establish a connection to the SQLite database\n",
    "        conn = self.conn\n",
    "        c = conn.cursor()\n",
    "        # Define query to delete edges with weight lower than the specified 'limit'\n",
    "        c.execute('DELETE FROM edgelist WHERE weight < ?', (limit,))\n",
    "        # Commit changes to the database\n",
    "        conn.commit()\n",
    "\n",
    "    # Function to retrieve the graph's number of edges\n",
    "    def number_of_edges(self):\n",
    "        # Define SELECT query to count the number of rows in the 'edgelist' table\n",
    "        query = 'SELECT COUNT(*) FROM edgelist'\n",
    "        # Call the function to run the SELECT query on the database and retrieve the result\n",
    "        result = self.query_graph(query)\n",
    "        # Return the number of edges, which is extracted from the query result\n",
    "        return result[0][0]\n",
    "\n",
    "    # Function to retrieve the graph's number of nodes\n",
    "    def number_of_nodes(self):\n",
    "        # Define SELECT query to count the number of rows in the 'nodelist' table\n",
    "        query = 'SELECT COUNT(*) FROM nodelist'\n",
    "        # Call the function to run the SELECT query on the database and retrieve the result\n",
    "        result = self.query_graph(query)\n",
    "        # Return the number of nodes, which is extracted from the query result\n",
    "        return result[0][0]\n",
    "\n",
    "    # Method to close the connection established with the SQLite database\n",
    "    def close_connection(self):\n",
    "        # Close the connection to the SQLite database\n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df67091",
   "metadata": {},
   "source": [
    "Then, the environment to create and work with an SQLite-based graph is set by initializing an instance of the `SQLiteGraph` class, defining the path to the SQLite Graph Database file as 'my_graph_database.db' in the same directory as the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e7965e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the Meta-Blocking Graph...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the SQLite Graph Database file path\n",
    "db_file = 'my_graph_database.db'\n",
    "print(\"Creating the Meta-Blocking Graph...\\n\")\n",
    "# Create the graph by initializing an instance of the SQLiteGraph class\n",
    "graph = SQLiteGraph(db_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13e8707",
   "metadata": {},
   "source": [
    "Then, the Graph Building Phase begins, which involves creating the graph structure by adding nodes and edges based on the information obtained from the token blocking operation. This phase runs for approximately 10-20 minutes depending on computer's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de877f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Graph Building Phase: Adding all nodes (entities) and edges (comparisons) to the graph...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2de972a70ef48fd8dd012a7d3790432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/392309099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Phase 1: Graph Building\n",
    "print(\"1. Graph Building Phase: Adding all nodes (entities) and edges (comparisons) to the graph...\")\n",
    "# Add nodes and edges to the graph using the blocking index generated from token blocking\n",
    "graph.add_nodes_and_edges_from_token_blocking_dict(block_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf130b",
   "metadata": {},
   "source": [
    "Then, in order to check that the previous phase completed successfully, the number of edges (comparisons) in the graph is compared with the previously calculated `num_comparisons` and the result of the comparison is printed as a boolean value, indicating whether the number of graph edges is equal to the number of comparisons in Task B. If the value is True, then the previous phase succeeded inserting all edges (comparisons) to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aec11fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Check: Is the number of graph edges the same as the number of comparisons of Task B?:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check if the number of graph edges is the same as the number of comparisons in Task B\n",
    "print('\\nCheck: Is the number of graph edges the same as the number of comparisons of Task B?:')\n",
    "print(graph.is_num_edges_equal_to(num_comparisons))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53fc55b",
   "metadata": {},
   "source": [
    "Then, the commencement of the Edge Weighting Phase is indicated, which invokes the `apply_cbs_weighting_scheme()` method on the `graph` instance to apply the Common Blocks Scheme (CBS) Weighting Scheme to the graph's edges. This phase runs for approximately 15-30 minutes depending on computer's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27dc3ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Edge Weighting Phase: Applying CBS Weighting Scheme...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e610334fa4f64504901a7c97fdf05735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Phase 2: Edge Weighting\n",
    "print(\"\\n2. Edge Weighting Phase: Applying CBS Weighting Scheme...\")\n",
    "# Apply the CBS Weighting Scheme to the graph\n",
    "graph.apply_cbs_weighting_scheme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7093b58",
   "metadata": {},
   "source": [
    "Then, the Graph Pruning Phase starts, which invokes the `prune_edges_with_weight_lower_than(threshold)` method on the `graph` instance to remove edges from the graph that have a weight lower than the defined `threshold`, which in our case is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9c9cf7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Graph Pruning Phase: Pruning edges with weight < 2 ...\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "# Phase 3: Graph Pruning\n",
    "# Set a threshold for pruning edges with weight below this value\n",
    "threshold = 2\n",
    "print(\"\\n3. Graph Pruning Phase: Pruning edges with weight <\", threshold, \"...\")\n",
    "# Remove edges from the graph that have weight less than the specified threshold\n",
    "graph.prune_edges_with_weight_lower_than(threshold)\n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ef8063",
   "metadata": {},
   "source": [
    "Then, in order to check that the previous phase completed successfully, the number of edges (comparisons) in the graph is again compared with the previously calculated `num_comparisons`. The value must be False, if the previous phase succeeded pruning all edges (comparisons) of the graph that have weight < 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeed273f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: Is the number of graph edges (after pruning) the same as the initial number of comparisons (Task B)?:\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check if the number of graph edges (after pruning) is the same as the initial number of comparisons (Task B)\n",
    "print('Check: Is the number of graph edges (after pruning) the same as the initial number of comparisons (Task B)?:')\n",
    "print(graph.is_num_edges_equal_to(num_comparisons))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa20cbe",
   "metadata": {},
   "source": [
    "Lastly, the start of the Block Collecting Phase is indicated, which invokes the `number_of_edges()` method on the `graph` instance to count the number of edges remaining in the graph after the pruning process. The final number of comparisons (after edge pruning) is 53,626,150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12245c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Block Collecting Phase: Count every retained edge...\n",
      "Final number of comparisons (after edge pruning): 53626150\n"
     ]
    }
   ],
   "source": [
    "# Phase 4: Block Collecting\n",
    "print(\"\\n4. Block Collecting Phase: Count every retained edge...\")\n",
    "# Count the number of edges after pruning to get the final number of comparisons\n",
    "print('Final number of comparisons (after edge pruning):', graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ad3f3",
   "metadata": {},
   "source": [
    "Finally, the `close_connection()` method on the `graph` instance is invoked to terminate the connection to the graph's SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f78200ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Closing connection with the graph...\n",
      "\n",
      "Closed!\n"
     ]
    }
   ],
   "source": [
    "# Close the connection to the graph\n",
    "print(\"\\nClosing connection with the graph...\\n\")\n",
    "graph.close_connection()\n",
    "print(\"Closed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518e4cdc",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bedfade",
   "metadata": {},
   "source": [
    "## Task D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3255dd74",
   "metadata": {},
   "source": [
    "In Task D, a function named `jaccard_similarity` is defined that calculates the **Jaccard similarity** between two entities based on their titles in a given DataFrame. The function begins by accepting three inputs: a DataFrame (`dataframe`), and two entity IDs (`id1` and `id2`). This function employs the following steps:\n",
    "\n",
    "1. <u>Title Extraction and Preprocessing</u>: The function extracts the titles of the entities associated with the provided IDs from the DataFrame. It converts the titles to lowercase and tokenizes them by splitting them into sets of words.\n",
    "\n",
    "2. <u>Intersection and Union Calculation</u>: The function calculates the intersection of the two sets (common words) and the union of the sets (total unique words). These values are essential for computing the Jaccard similarity.\n",
    "\n",
    "3. <u>Jaccard Similarity Computation</u>: The Jaccard similarity is computed by dividing the intersection count by the union count. This ratio quantifies the similarity between the sets of words, which correspond to the titles of the two entities.\n",
    "\n",
    "4. <u>Result Return</u>: The calculated Jaccard similarity value is returned as the output of the function, representing the extent of similarity between the titles of the two entities ranging from 0 (no similar word) to 1 (identical).\n",
    "\n",
    "Overall, the function enables the assessment of title-based similarity between entities by systematically preprocessing titles, computing set-based measures, and generating a Jaccard similarity value as the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b81a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that calculates the Jaccard similarity between two entities in a DataFrame based on their titles,\n",
    "# given their IDs\n",
    "def jaccard_similarity(dataframe, id1, id2):\n",
    "    # Extract the titles of entities with ID1 and ID2 from the DataFrame\n",
    "    title1 = dataframe.loc[dataframe['id'] == id1, 'title']\n",
    "    title2 = dataframe.loc[dataframe['id'] == id2, 'title']\n",
    "    # Convert the titles to lowercase and split them into sets of words\n",
    "    set1 = set(str(title1.values[0]).lower().split())\n",
    "    set2 = set(str(title2.values[0]).lower().split())\n",
    "    # Calculate the intersection of the two sets (common words)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    # Calculate the union of the two sets (total unique words)\n",
    "    union = len(set1.union(set2))\n",
    "    # Calculate the Jaccard similarity by dividing the intersection by the union\n",
    "    jaccard_sim = intersection / union\n",
    "    # Return the Jaccard similarity value\n",
    "    return jaccard_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45474491",
   "metadata": {},
   "source": [
    "The `jaccard_similarity` function is then tested by calculating the Jaccard similarity of entities with IDs 10 and 810. The result of the computation is displayed using the `print` statement. This action demonstrates the usage of the function to determine the Jaccard similarity between the titles of the entities associated with the given IDs (10 and 810) in the provided DataFrame (`df`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0af232ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Jaccard similarity of entities with ID 10 and 810 is:\n",
      "0.2\n",
      "\n",
      "--------------------------\n",
      "END\n",
      "BYE-BYE\n"
     ]
    }
   ],
   "source": [
    "# Test the function by calculating the Jaccard similarity of entities with ID 10 and 810\n",
    "print(\"The Jaccard similarity of entities with ID 10 and 810 is:\")\n",
    "print(jaccard_similarity(df, 10, 810))\n",
    "print(\"\\n--------------------------\")\n",
    "print(\"END\")\n",
    "print(\"BYE-BYE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
